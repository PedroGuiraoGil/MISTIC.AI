name: Actualizar EPG

on:
  schedule:
    - cron: '0 8,12,16,20 * * *'
  workflow_dispatch:

jobs:
  update-epg:
    runs-on: ubuntu-latest
    permissions:
      contents: write

    steps:
      - name: Checkout repo
        uses: actions/checkout@v4

      - name: Instalar dependencias Python
        run: |
          pip install requests beautifulsoup4 playwright -q
          python -m playwright install chromium --with-deps

      - name: Scrapear programacion movistarplus.es
        run: |
          python3 << 'PYEOF'
          import json, re, sys
          from datetime import datetime, timedelta
          import requests
          from bs4 import BeautifulSoup

          # ── Canales: slug movistar → nombre mostrado ─────────────────────────
          CANALES = {
              'mplus':   'Movistar Plus+',
              'vamosd':  'M+ Vamos',
              'laliga':  'M+ LaLiga',
              'dptv':    'M+ Deportes',
              'tve':     'La 1',
              'la2':     'La 2',
              'a3':      'Antena 3',
              't5':      'Telecinco',
              'c4':      'Cuatro',
              'sexta':   'La Sexta',
              'neox':    'Neox',
              'nova':    'Nova',
              'mega':    'Mega',
              'energy':  'Energy',
              'fdf':     'FDF',
              'clan':    'Clan',
          }

          TODAY = datetime.now().strftime('%Y-%m-%d')
          BASE  = 'https://www.movistarplus.es'
          HEADERS = {
              'User-Agent': 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 Chrome/120 Safari/537.36',
              'Accept': 'text/html,application/xhtml+xml,*/*',
              'Accept-Language': 'es-ES,es;q=0.9',
          }

          def parse_next_data(html):
              """Extrae programas del JSON embebido __NEXT_DATA__ (sin JS)."""
              m = re.search(r'<script id="__NEXT_DATA__"[^>]*>(.*?)</script>', html, re.DOTALL)
              if not m:
                  return None
              try:
                  data = json.loads(m.group(1))
              except Exception:
                  return None

              # Buscar el array de programas en cualquier parte del árbol
              def buscar_progs(obj, depth=0):
                  if depth > 10: return None
                  if isinstance(obj, list) and len(obj) > 2:
                      # ¿Es lista de programas? Buscar objetos con hora y titulo
                      if all(isinstance(x, dict) for x in obj[:3]):
                          keys = set().union(*[x.keys() for x in obj[:3]])
                          if any(k in keys for k in ['startTime','start_time','hora','time','inicio']):
                              return obj
                  if isinstance(obj, dict):
                      for v in obj.values():
                          r = buscar_progs(v, depth+1)
                          if r: return r
                  if isinstance(obj, list):
                      for item in obj:
                          r = buscar_progs(item, depth+1)
                          if r: return r
                  return None

              progs_raw = buscar_progs(data)
              if not progs_raw:
                  return None

              result = []
              for p in progs_raw:
                  # Intentar extraer hora y título con múltiples nombres de campo posibles
                  hora = (p.get('startTime') or p.get('start_time') or p.get('hora') or
                          p.get('time') or p.get('inicio') or p.get('horaInicio') or '')
                  titulo = (p.get('title') or p.get('titulo') or p.get('name') or
                            p.get('nombre') or p.get('programTitle') or '')
                  hora_fin = (p.get('endTime') or p.get('end_time') or p.get('fin') or
                              p.get('horaFin') or '')
                  if hora and titulo:
                      # Normalizar hora a HH:MM
                      hora_clean = re.search(r'\d{2}:\d{2}', str(hora))
                      fin_clean  = re.search(r'\d{2}:\d{2}', str(hora_fin))
                      result.append({
                          'inicio': hora_clean.group() if hora_clean else str(hora)[:5],
                          'fin':    fin_clean.group()  if fin_clean  else None,
                          'titulo': str(titulo).strip(),
                          'desc':   str(p.get('description') or p.get('descripcion') or '')[:120].strip(),
                      })
              return result if result else None

          def parse_html_fallback(html, slug):
              """Parsea el HTML renderizado cuando __NEXT_DATA__ no da programas."""
              soup = BeautifulSoup(html, 'html.parser')
              progs = []

              # movistarplus usa clases como schedule__item, program-item, etc.
              selectors = [
                  'li[class*="schedule"]', 'div[class*="schedule-item"]',
                  'li[class*="program"]',  'div[class*="program-item"]',
                  'article[class*="program"]', 'div[class*="epg-item"]',
                  '.schedule__item', '.programacion__item', '.guia__item',
              ]
              items = []
              for sel in selectors:
                  items = soup.select(sel)
                  if items: break

              for item in items:
                  # Hora: buscar span/time con formato HH:MM
                  hora_el = item.find(class_=re.compile(r'hour|hora|time|inicio', re.I))
                  if not hora_el:
                      hora_el = item.find('time')
                  hora_txt = hora_el.get_text(strip=True) if hora_el else ''
                  hora_m = re.search(r'\d{1,2}:\d{2}', hora_txt)

                  # Título
                  tit_el = item.find(class_=re.compile(r'title|titulo|name|nombre', re.I))
                  if not tit_el:
                      tit_el = item.find(['h2','h3','h4','strong','span'], recursive=False)
                  titulo = tit_el.get_text(strip=True) if tit_el else item.get_text(' ', strip=True)[:60]

                  if hora_m and titulo and len(titulo) > 2:
                      progs.append({
                          'inicio': hora_m.group(),
                          'fin': None,
                          'titulo': titulo,
                          'desc': '',
                      })
              return progs

          def scrapear_canal_requests(slug):
              url = f'{BASE}/programacion-tv/{slug}/{TODAY}'
              try:
                  r = requests.get(url, headers=HEADERS, timeout=20)
                  r.raise_for_status()
                  html = r.text
                  progs = parse_next_data(html)
                  if progs:
                      print(f'  [__NEXT_DATA__] {slug}: {len(progs)} programas')
                      return progs
                  progs = parse_html_fallback(html, slug)
                  if progs:
                      print(f'  [HTML] {slug}: {len(progs)} programas')
                      return progs
                  print(f'  [requests] {slug}: sin datos en HTML estático')
                  return None
              except Exception as e:
                  print(f'  [requests] {slug} ERROR: {e}')
                  return None

          def scrapear_canal_playwright(slug):
              from playwright.sync_api import sync_playwright
              url = f'{BASE}/programacion-tv/{slug}/{TODAY}'
              try:
                  with sync_playwright() as p:
                      browser = p.chromium.launch(headless=True)
                      page = browser.new_page()
                      page.set_extra_http_headers({'Accept-Language': 'es-ES,es;q=0.9'})
                      page.goto(url, wait_until='networkidle', timeout=30000)
                      # Esperar a que aparezcan programas
                      try:
                          page.wait_for_selector(
                              '[class*="schedule"],[class*="program"],[class*="epg"],[class*="guia"]',
                              timeout=10000
                          )
                      except Exception:
                          pass
                      html = page.content()
                      browser.close()

                  progs = parse_next_data(html)
                  if progs:
                      print(f'  [PW+NEXT_DATA] {slug}: {len(progs)} programas')
                      return progs
                  progs = parse_html_fallback(html, slug)
                  if progs:
                      print(f'  [PW+HTML] {slug}: {len(progs)} programas')
                      return progs
                  print(f'  [PW] {slug}: sin datos incluso con JS')
                  # Debug: guardar HTML para inspección
                  with open(f'/tmp/debug_{slug}.html', 'w') as f:
                      f.write(html[:5000])
                  return None
              except Exception as e:
                  print(f'  [PW] {slug} ERROR: {e}')
                  return None

          # ── Main ─────────────────────────────────────────────────────────────
          result = {}
          playwright_needed = []

          # Primero intento rápido con requests
          for slug, label in CANALES.items():
              progs = scrapear_canal_requests(slug)
              if progs:
                  progs.sort(key=lambda x: x['inicio'])
                  result[label] = progs
              else:
                  playwright_needed.append((slug, label))

          # Los que fallaron → Playwright
          if playwright_needed:
              print(f'\nUsando Playwright para {len(playwright_needed)} canales...')
              for slug, label in playwright_needed:
                  progs = scrapear_canal_playwright(slug)
                  if progs:
                      progs.sort(key=lambda x: x['inicio'])
                      result[label] = progs
                  else:
                      result[label] = []

          # Rellenar vacíos
          for label in CANALES.values():
              if label not in result:
                  result[label] = []

          # Resumen
          print('\n── Resultado ──────────────────────────────')
          for label, progs in result.items():
              if progs:
                  print(f'  ✅ {label}: {len(progs)} programas — primero: {progs[0]["inicio"]} {progs[0]["titulo"]}')
              else:
                  print(f'  ❌ {label}: VACÍO')

          total = sum(len(v) for v in result.values())
          if total == 0:
              print('\nATENCIÓN: 0 programas obtenidos — se mantiene el epg.json anterior')
              sys.exit(0)

          with open('epg.json', 'w', encoding='utf-8') as f:
              json.dump({
                  'actualizado': datetime.utcnow().strftime('%Y-%m-%dT%H:%M:%SZ'),
                  'fuente': 'movistarplus.es',
                  'canales': result,
              }, f, ensure_ascii=False, indent=2)

          print(f'\n✅ epg.json guardado: {total} programas')
          PYEOF

      - name: Commit y push
        run: |
          git config user.name  "github-actions[bot]"
          git config user.email "github-actions[bot]@users.noreply.github.com"
          git add epg.json
          git diff --staged --quiet || git commit -m "EPG movistar $(date -u '+%Y-%m-%d %H:%M UTC')"
          git push
